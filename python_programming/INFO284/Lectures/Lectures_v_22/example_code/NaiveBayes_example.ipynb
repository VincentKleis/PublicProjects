{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "ran = 2345\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris_dataset['data'])\n",
    "X_scaled = scaler.transform(iris_dataset['data'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, iris_dataset['target'], random_state=ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5,weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbc = GaussianNB()\n",
    "nbc.fit(X_train, y_train)\n",
    "nbc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from: https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "\n",
    "Dataset can be downloaded here: http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sinbt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = load_files(r\"O:\\undervisning\\info284\\info284_2021\\data\\txt_sentoken\")\n",
    "X, y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'we could paraphrase michelle pfieffer\\'s character in dangerous minds and say that beyond rangoon starts with an \" a . \" \\nthat\\'s fair enough . \\nall movies , like all school children , should be given the benefit of the doubt . \\nthe chance to succeed . \\nafter all , we like to think that the right combination of talent and effort can do wonders . \\nmountains can be moved , and good movies can be made . \\nyeah right . \\nchildren fail , as do films . \\nas does director john boorman\\'s latest . \\nthe success of beyond rangoon hinges on the believability of patricia arquette ( ed wood , true romance ) as the busty westerner-in-peril wandering about 1988 burma without a passport . \\nthough we can stomach the mild plot contrivances that get her there , it\\'s a tougher task to overlook the actress . \\nshe\\'s a lightweight . \\nas the first scene ( with narration ! ) \\ndemonstrates , she doesn\\'t have * nearly * enough range for the emotions that her character--a mother fleeing the memories of a murdered husband and son--is supposed to show . \\nshe may give a stronger performance than , say , keanu reeves in a walk in the clouds , by not by much . \\nbeyond rangoon is a very physically appealing film , thanks to the practiced craftsmanship of john boorman ( deliverance , excalibur , hope and glory ) . \\nhe keeps the narrative moving , no matter how muddy the story--or the heroine--gets . \\nwhy he chose arquette remains a mystery , though . \\nmaybe he was thinking that the dramatic weight of the story would overcome any casting deficiencies . \\nbut even after an hour of half-stated political statements and murky mass killings , we * still * don\\'t know enough of what\\'s happening in this country to feel distressed for the characters or their situations . \\nempty exoticism . \\nthe technical credits in beyond rangoon are a curious mix , combining lush jungle photography with bad blue-screen work . \\nalso odd is the obvious dubbing . \\ndone to make some of the foreign characters sound less foreign ? \\nand , is it my mistake , or do the same extras keep reappearing as different soldiers ? ? \\ndoo doo doo doo . \\n[ \" twilight zone \" theme , or opinion of movie . \\nyou be the judge . ] \\n' 0\n"
     ]
    }
   ],
   "source": [
    "print(X[3], y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sinbt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_pre = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pre, y, test_size=0.2, random_state=ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnbc = MultinomialNB()\n",
    "mnbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mnbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[166  43]\n",
      " [ 37 154]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81       209\n",
      "           1       0.78      0.81      0.79       191\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[170  39]\n",
      " [ 48 143]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80       209\n",
      "           1       0.79      0.75      0.77       191\n",
      "\n",
      "    accuracy                           0.78       400\n",
      "   macro avg       0.78      0.78      0.78       400\n",
      "weighted avg       0.78      0.78      0.78       400\n",
      "\n",
      "0.7825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bin_vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'), binary=True)\n",
    "X_pre_bin = bin_vectorizer.fit_transform(documents).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pre_bin, y, test_size=0.2, random_state=ran)\n",
    "bnbc = BernoulliNB()\n",
    "bnbc.fit(X_train,y_train)\n",
    "y_pred = bnbc.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
